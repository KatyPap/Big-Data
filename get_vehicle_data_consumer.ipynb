{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to kafka broker and create Consumer, only used initialy to test if consuming from kafka topic works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first set up the and run the kafka server also install kafka-python. The commands can be found in 'kakfa commands.txt'\n",
    "\n",
    "# from kafka import KafkaAdminClient\n",
    "# from kafka import KafkaConsumer, TopicPartition\n",
    "# import json\n",
    "\n",
    "# topic_name = 'vehicle_positions'\n",
    "# consumer = KafkaConsumer(#topic = topic_name,\n",
    "#                          bootstrap_servers=['localhost:9092'],\n",
    "#                          auto_offset_reset='earliest', #will start consuming from the first message in the topic \n",
    "#                          value_deserializer = lambda x: json.loads(x.decode(\"utf-8\")),\n",
    "#                           consumer_timeout_ms=10000 #stop consumer from waiting for messages after 10000ms \n",
    "#                          )\n",
    "\n",
    "# tp1 = TopicPartition(topic_name, 0)\n",
    "# consumer.assign([tp1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Consumer, only used initialy to test if consuming from kafka topic works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for message in consumer: #consumer will bring all events from the start end then wait for the next event to happend untill it time outs after 10000ms \n",
    "#     # print(f\"Received Value: {message.value}, Key:{message.key}, offset: {message.offset}, partition:{message.partition}\")\n",
    "#     print(message.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Injest Data with spark from kafka and send data to mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# connect to mongo and clear existing collection\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['your_database']\n",
    "kafka_aggregated_data = db['kafka_aggregated_data'] #prosessed agregated data from kafka\n",
    "kafka_raw_data = db['kafka_raw_data'] #unprosessed raw data from kafka broker\n",
    "kafka_aggregated_data.drop() #delete if exists \n",
    "kafka_raw_data.drop() #delete if exists \n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import from_json, col, to_timestamp, count, avg, desc, window, concat, lit, date_format\n",
    "\n",
    "\n",
    "def process_aggregated(df, batch_id):\n",
    "    print(f\"Batch aggregated {batch_id} written to MongoDB. Records: {df.count()}\")\n",
    "    df.show()\n",
    "    \n",
    "    # Convert timestamp to string to avoid MongoDB timestamp issues\n",
    "    mongo_df = df.withColumn(\"window_start\", col(\"window_start\").cast(\"string\")) \\\n",
    "                .withColumn(\"window_end\", col(\"window_end\").cast(\"string\"))\n",
    "\n",
    "    # Write to MongoDB and replace the document if a new updated document comes \n",
    "    mongo_df.write \\\n",
    "        .format(\"mongodb\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"operationType\", \"update\") \\\n",
    "        .option(\"upsertDocument\", \"true\") \\\n",
    "        .option(\"database\", \"your_database\") \\\n",
    "        .option(\"collection\", \"kafka_aggregated_data\") \\\n",
    "        .save()\n",
    "\n",
    "\n",
    "def process_raw(df, batch_id):\n",
    "    print(f\"Batch raw {batch_id} written to MongoDB. Records: {df.count()}\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    # Write to MongoDB and replace the document if a new updated document comes \n",
    "    df.write \\\n",
    "        .format(\"mongodb\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"operationType\", \"update\") \\\n",
    "        .option(\"upsertDocument\", \"true\") \\\n",
    "        .option(\"database\", \"your_database\") \\\n",
    "        .option(\"collection\", \"kafka_raw_data\") \\\n",
    "        .save()\n",
    "\n",
    "\n",
    "topic_name = 'vehicle_positions'\n",
    "\n",
    "# Create Spark Session with both Kafka and MongoDB packages\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaStreaming\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,\" + \n",
    "            \"org.mongodb.spark:mongo-spark-connector_2.12:10.3.0\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/\") \\\n",
    "    .config(\"spark.mongodb.output.uri\",  \"mongodb://127.0.0.1:27017/\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Raw Data\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"origin\", StringType(), True),\n",
    "    StructField(\"destination\", StringType(), True),\n",
    "    StructField(\"time\", StringType(), True),\n",
    "    StructField(\"link\", StringType(), True),\n",
    "    StructField(\"position\", DoubleType(), True),\n",
    "    StructField(\"spacing\", DoubleType(), True),\n",
    "    StructField(\"speed\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Read from Kafka earliest or latest\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", topic_name) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "    \n",
    "raw_df = kafka_df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    ").select(\"data.*\") \\\n",
    ".withColumn(\"time\", to_timestamp(col(\"time\"), \"dd/MM/yyyy HH:mm:ss\"))\n",
    "\n",
    "# Process the data with correct aggregation syntax\n",
    "processed_df = raw_df \\\n",
    "    .withWatermark(\"time\" , \"30 seconds\") \\\n",
    "    .groupBy(window(\"time\", \"10 seconds\"),\n",
    "             \"link\") \\\n",
    "    .agg(\n",
    "        count(\"speed\").alias(\"total_vehicles\"),  # Using count on a specific column\n",
    "        avg(\"speed\").alias(\"average_speed\")      # Using avg function directly\n",
    "     ) \\\n",
    "    .select(\n",
    "            concat(col(\"window.start\"), lit(\"_\"), col(\"window.end\"), lit(\"_\"), col(\"link\")).alias(\"_id\"),\n",
    "            col(\"window.start\").alias(\"window_start\"),\n",
    "            col(\"window.end\").alias(\"window_end\"),  \n",
    "            \"link\",\n",
    "            \"total_vehicles\",\n",
    "            \"average_speed\"\n",
    "        ) \n",
    "\n",
    "# Write output to console for testing\n",
    "query_prosessed = processed_df.writeStream \\\n",
    "    .foreachBatch(process_aggregated) \\\n",
    "    .queryName(\"processed_data\") \\\n",
    "    .start()\n",
    "\n",
    "raw_df_with_id = raw_df \\\n",
    "        .select(\n",
    "            concat(col(\"name\"), lit(\"_\"), col(\"time\")).alias(\"_id\"),\n",
    "            \"name\",\n",
    "            \"origin\",\n",
    "            \"destination\",\n",
    "            date_format(col(\"time\"), \"yyyy-MM-dd HH:mm:ss\").alias(\"time\"),  # Convert timestamp to string\n",
    "            \"link\",\n",
    "            \"position\",\n",
    "            \"spacing\",\n",
    "            \"speed\"\n",
    "        ) \n",
    "        \n",
    "# # Write raw data to console\n",
    "raw_query = raw_df_with_id.writeStream \\\n",
    "    .foreachBatch(process_raw) \\\n",
    "    .queryName(\"raw_data\") \\\n",
    "    .start()\n",
    "    \n",
    "\n",
    "# Keep the application running until both streams terminate\n",
    "# spark.streams.awaitAnyTermination()\n",
    "query_prosessed.awaitTermination()\n",
    "raw_query.awaitTermination()\n",
    "\n",
    "# query2.awaitTermination()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code notes\n",
    "\n",
    "# # Write output to console for testing\n",
    "# query_prosessed = processed_df.writeStream \\\n",
    "#     .foreachBatch(process_batch) \\\n",
    "#     .queryName(\"processed_data\") \\\n",
    "#     .start()\n",
    "# # Each batch will contain:\n",
    "# # - New windows that were created\n",
    "# # - Updated windows that changed due to late data (within watermark)\n",
    "# # - Does NOT include unchanged windows\n",
    "\n",
    "#     # .foreach(process_row) \\\n",
    "#     # .foreachBatch(process_batch) \\\n",
    "\n",
    "#     # .outputMode(\"update\") \\\n",
    "#     # .format(\"console\") \\\n",
    "#     # .option(\"truncate\", \"false\") \\\n",
    "#     # .queryName(\"processed_data\") \\\n",
    "#     # .trigger(processingTime='5 seconds') \\\n",
    "#     # .start()\n",
    "\n",
    "\n",
    "# # # Write raw data to console\n",
    "# raw_query = raw_df.writeStream \\\n",
    "#     .foreach(process_row) \\\n",
    "#     .queryName(\"processed_data\") \\\n",
    "#     .start()\n",
    "    \n",
    "#     # .outputMode(\"update\") \\\n",
    "#     # .format(\"console\") \\\n",
    "#     # .queryName(\"raw_data\") \\\n",
    "#     # .start()\n",
    "\n",
    "# # Keep the application running until both streams terminate\n",
    "# spark.streams.awaitAnyTermination()\n",
    "# # # Keep the application running\n",
    "# # query_1.awaitTermination()\n",
    "\n",
    "\n",
    "# # Process the data with correct aggregation syntax\n",
    "# processed_df = raw_df \\\n",
    "#     .withWatermark(\"time\" , \"30 seconds\") \\\n",
    "#     .groupBy(window(\"time\", \"10 seconds\"),\n",
    "#              \"link\") \\\n",
    "#     .agg(\n",
    "#         count(\"speed\").alias(\"total_vehicles\"),  # Using count on a specific column\n",
    "#         avg(\"speed\").alias(\"average_speed\")      # Using avg function directly\n",
    "#      ) \\\n",
    "#     .select(\n",
    "#             col(\"window.start\").alias(\"window_start\"),\n",
    "#             col(\"window.end\").alias(\"window_end\"),  \n",
    "#             \"link\",\n",
    "#             \"total_vehicles\",\n",
    "#             \"average_speed\"\n",
    "#         ) \n",
    "#         #  .orderBy(desc(\"window_start\"), \"link\")        \n",
    "#     # .orderBy(desc(col(\"time_window.start\")), \"link\")  # Order by grouping columns\n",
    "\n",
    "\n",
    "# .trigger(processingTime='5 seconds')\n",
    "# This means:\n",
    "# Spark checks for new data every 5 seconds\n",
    "# If there is new data, it processes all accumulated data since the last trigger\n",
    "# If no new data, it waits until next trigger\n",
    "# The aggregations (groupBy, count, avg) are computed for ALL data received so far because you're using outputMode(\"complete\")\n",
    "\n",
    "# # Process every 5 seconds (your current setting)\n",
    "# .trigger(processingTime='5 seconds')\n",
    "# # Process as soon as new data arrives\n",
    "# .trigger(availableNow=True)\n",
    "# # Process once and stop\n",
    "# .trigger(once=True)\n",
    "# # Continuous processing (micro-batch as small as 1ms)\n",
    "# .trigger(continuous='1 second')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Complete mode (your current setting)\n",
    "# .outputMode(\"complete\")\n",
    "# # - Recalculates ALL aggregations every trigger\n",
    "# # - Shows full result table each time\n",
    "# # - Good for totals/averages over all time\n",
    "\n",
    "# # Update mode\n",
    "# .outputMode(\"update\")\n",
    "# # - Only outputs changed aggregations\n",
    "# # - Good for dashboards/live updates\n",
    "\n",
    "# # Append mode\n",
    "# .outputMode(\"append\")\n",
    "# # - Only outputs new aggregations\n",
    "# the only one that can be used without agregation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condapy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
